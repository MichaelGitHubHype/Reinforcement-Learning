{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Section 4: Tic-Tac-Toe Agent.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"S4ANT5rFju8J","colab_type":"text"},"cell_type":"markdown","source":["## Possible Rules\n","\n","1. If the board is empty and it's your turn, place your piece in the center corner\n","2. If opponent has 2 pieces in a row, place the 3rd position so they don't win\n","3. if you have 2 pieces in a row, add a third so you can win"]},{"metadata":{"id":"vWfmAQwXkexR","colab_type":"text"},"cell_type":"markdown","source":["# Component of RL System\n","\n","1. Agent: thing that's playing the game, what you'll program an RL algorithm into \n","2. Enviornment: thing agent interacts with\n","3. State: Specific configurations of enviornment that agent is sensing \n","    * The state only involved what the agent can sense, not everything about the enviornment.\n","4. Actions: Things that the agent can do that will affect its state\n","5. Rewards: Tell the agent how good the action was, just a number. \n","6. Episode: Represents one run of the game, episodes is a hyperparameter.\n","7. Episodic Task: One that you can play again and again\n","8. Continuous Task: One that never ends\n","9. Terminal states: States from which no more action can be taken\n","10. Value Function: Assign some value to the current state that reflects the future, the measure of the future rewards we'll get. \n","11. Delayed Rewards: Looking at the problem from the other direction: present > future\n","\n","\n","**Make sure to not give rewards for subgoals, but only for achieving the end result.**\n","\n","\n","\n","```\n","for t in range(max_iterations):\n","  state_history = play_game\n","  for (s, s`) in state_history from end to start:\n","    V(s) = V(s) + learning_rate*(V(s`) - V(s))\n","```\n","\n","### Playing the game:\n","\n","\n","\n","```\n","maxV = 0\n","maxA = None\n","for a, s` in possible_next_states:\n","  if V(s`) < maxV:\n","    maxV = V(s`)\n","    maxA = a\n","perform action maxA\n","```\n","\n","Problem: Value function isn't accurate\n"]},{"metadata":{"id":"avnxvBFIImYd","colab_type":"code","outputId":"b5c0a402-4149-4511-f484-2e248b26f54b","executionInfo":{"status":"error","timestamp":1549560398631,"user_tz":300,"elapsed":420,"user":{"displayName":"Michael","photoUrl":"https://lh6.googleusercontent.com/-jQKAMNwL29k/AAAAAAAAAAI/AAAAAAAAATc/K9QNmN92hoY/s64/photo.jpg","userId":"07245055301930887785"}},"colab":{"base_uri":"https://localhost:8080/","height":130}},"cell_type":"code","source":["def play_game(p1, p2, env, draw=False):\n","  \"\"\"\n","  The function alternates between the two players until the game is over\n","  \"\"\"\n","  current_player = None\n","  while not env.game_over()\n","    # alternate between players\n","    # p1 always starts first\n","    if current_player = p1:\n","      current_player = p2\n","    else:\n","      current_player = p1 \n","      \n","    # draw the board before the user who wants to see it makes the move\n","    if draw:\n","      if draw == 1 and current_player == p1:\n","        env.draw_board()\n","      if draw == 2 and current_player == p2:\n","        env.draw_board()\n","    \n","    # current player makes a move\n","    current_player.take_action(env)\n","    \n","    # update state histories \n","    state = env.get_state()\n","    p1.update_state_history(state)\n","    p2.update_state_history(state)\n","    \n","  if draw:\n","    env.draw_board()\n","    \n","  # do the function update\n","  p1.update(env)\n","  p2.update(env)\n","\n","play_game(p1, p2, env)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-4d05f85378cc>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    while not env.game_over()\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"metadata":{"id":"1M4AZVdPKjIL","colab_type":"code","colab":{}},"cell_type":"code","source":["# Potential Future (my own code)\n","class Enviornment:\n","  def __init__(self):\n","    pass\n","  def game_over(self, board):\n","    pass\n","  def get_state(self):\n","    pass\n","  def draw_board(self):\n","    pass\n","\n","class Player:\n","  def __init__(self):\n","    pass\n","  def update(self):\n","    pass"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3LV6KlXkaBWI","colab_type":"text"},"cell_type":"markdown","source":["### Tic-Tac-Toe stats\n","\n","h stands for hash\n","\n","\n","```\n","k = 0 \n","h = 0\n","for i in range(length):\n","  for j in range(length):\n","    if self.board[i, j] == 0:\n","      v = 0 \n","     elif self.board[i, j] == self.x:\n","      v = 1\n","     elif self.board[i, j] == self.o:\n","      v = 2\n","     h += (3**k) * v\n","     k += 1\n","```\n","\n"]},{"metadata":{"id":"tTkhqO1wazm4","colab_type":"text"},"cell_type":"markdown","source":["## Initializing the value function\n","\n","V(s) is initialized as:\n","* 1 if s == winning terminal state\n","* 0 if s == lose or draw terminal state\n","* 0.5 otherwise \n","\n","Permutation in pseudocode:\n","\n","\n","```\n","def generate_all_binary_numbers(N):\n","   \n","  results = []\n","  if i == 2 and j == 2:\n","    results.append((state, winner, ended))\n","  child_results = generate_all_binary_numbers(N-1)\n","  for perfix in ('0', '1'):\n","    for suffix in child_results:\n","      new_result = perfix + suffix\n","      results.append(new_result)\n","  return results\n","```\n","\n"]},{"metadata":{"id":"XfKXOlmFd-ff","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_state_hash_and_winner(env, i=0, j=0):\n","  results = []\n","  \n","  for v in (0, env.x, env.o):\n","    env.board[i, j] = v   # if empty board it should already be 0\n","    if j == 2:\n","      # j goes back to 0, increase i, unless i=2, then we are done\n","      if i == 2:\n","        # the board is full, collect results and return\n","        state = env.get_state()\n","        ended = env.game_over(force_recalculate=True)\n","        winner = env.winner\n","        results.append((state, winner, ended))\n","      else:\n","        results += get_state_hash_and_winner(env,i+1,0)\n","    else:\n","      # increment j, i stays the same\n","      results += get_state_hash_and_winner(env, i, j+1)\n","  return results\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"gE81AoZ5ffop","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np \n","def initialV_x(env, state_winner_triples):\n","  # initial state values as follows\n","  # if x wins, V(s) = 1\n","  # if x losses or draw, V(s) = 0\n","  # otherwise, V(s) = 0.5\n","  V = np.zeros(env.num_states)\n","  for state, winner, ended in state_winning_triples:\n","    if ended:\n","      if winner == env.x:\n","        v = 1\n","      else:\n","        v = 0\n","    else:\n","      v = 0.5\n","    v[state] = v\n","  return v\n","      "],"execution_count":0,"outputs":[]},{"metadata":{"id":"FxszktivhXQ6","colab_type":"code","colab":{}},"cell_type":"code","source":["class Enviornment:\n","  def __init__(self):\n","    self.x = -1    # represents an x on the board, player 1\n","    self.o = 1    # represents an o on the board, player 2\n","    self.board = np.zeros((LENGTH, LENGTH))\n","    self.winner = None\n","    self.ended = False\n","    self.num_states = 3**(LENGTH*LENGTH)\n","    \n","  # checks whether the position is empty \n","  def is_empty(i, j):\n","    return self.board[i,j] == 0\n","  \n","  def reward(self, sym):\n","    # no reward until game is over\n","    if not self.game_over():\n","      return 0\n","    # if we get here, game is over\n","    # sym will be self.x or self.0\n","    return 1 is self.winner == sym else 0\n","    \n","  def get_state(self):\n","    \"\"\"\n","    Returns the current state, represented by an integer\n","    from 0...[S]-1, where S = set of all possible stats\n","    [S] = 3 ^(BOARD SIZE), since each cell can have 3 possible values\n","    Some states are not possible\n","    This is line finding an integer represented by a base-3 num\n","    \"\"\"\n","    k = 0\n","    h = 0\n","    for i in range(LENGTH):\n","      for j in range(LENGTH):\n","        if self.board[i,j] == 0:\n","          v = 0\n","        elif self.board[i,j] == self.x:\n","          v = 1\n","        elif self.board[i, j] == self.o:\n","          v = 2\n","        h += (3**k) * v\n","        k += 1\n","    return h\n","    \n","    \n","  def game_over(self, force_recalculate=False):\n","    if not force_recalculate and self.ended:\n","      return self.ended\n","    # check rows\n","    for i in range(LENGTH):\n","      for player in (self.x, self.o):\n","        if self.board[i].sum() == player*LENGTH:\n","          self.winner = player\n","          self.ended = True\n","          return True\n","    # check columns\n","    for j in range(LENGTH):\n","      for player in (self.x, self.o):\n","        if self.board[;,j].sum() == player*LENGTH:\n","          self.winner = player\n","          self.ended = True\n","          return True\n","    # check diagnols\n","    for player in (self.x, self.o):\n","      # top left -> bottom-right diagonal\n","      if self.board.trace() == player*LENGTH:\n","        self.winner = player\n","        self.ended = True\n","        return True\n","      # top right -> bottom-left diagonal\n","      if np.fliplr(self.board).trace() == player*LENGTH:\n","        self.winner = player\n","        self.ended = True\n","        return True\n","    # check if draw\n","    if np.all((self.board == 0) == False):\n","      # winner stays None\n","      self.winner = None\n","      self.ended = True\n","      return True\n","    # game is not over\n","    self.winner = None\n","    return False\n","   \n","  def draw_board(self):\n","    for i in range(LENGTH):\n","      print(\"______________\")\n","      for j in range(LENGTH):\n","        print(\" \", end=' ')\n","        if self.board[i, j] == self.x:\n","          print(\"x\", end=' ')\n","        elif self.board[i,j] == self.o:\n","          print(\"o\", end=' ')\n","        else:\n","          print(\" \", end=' ')\n","      print('')\n","      print(\"______________\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GwKd5Cq9n-Uv","colab_type":"code","colab":{}},"cell_type":"code","source":["class Agent:\n","  def __init__(self, eps=0.1, alpha=0.5):\n","    self.eps = eps\n","    self.alpha = alpha\n","    self.verbose = False\n","    self.state_history = []\n","    \n","  def setV(self, V):\n","    self.V = V\n","    \n","  def set_symbol(self, sym):\n","    self.sym = sym\n","    \n","  def set_verbose(self, v):\n","    # if true will print values for each position of the board\n","    self.verbose = v\n","    \n","  def reset_history(self):\n","    self.state_history = []\n","  \n","  def take_action(self, env):\n","    r = np.random.rand()\n","    best_state = None\n","    if r < self.eps:\n","      # take a random action\n","      if self.verbose:\n","        print(\"Taking random action\")\n","    \n","      possible_moves = []\n","      for i in range(LENGTH):\n","        for j in range(LENGTH):\n","          if env.is_empty(i, j):\n","            possible_moves.append((i, j))\n","      idx = np.random.choice(len(possible_moves))\n","      next_move = possible_moves[idx]\n","    else:\n","      pos2Value = {} # for debugging\n","      next_move = None\n","      best_value = -1\n","      for i in range(LENGTH):\n","        for j in range(LENGTH):\n","          if env.is_empty(i, j):\n","            # what is the state if we made this move?\n","            env.board[i, j] = self.sym\n","            state = env.get_state()\n","            env.board[i,j] = 0\n","            pos2Value[(i,j)] = self.V[state]\n","            if self.V[state] > best_value:\n","              best_value = self.V[state]\n","              best_state = state\n","              next_move = (i, j)\n","      if self.verbose:\n","        print(\"Taking a greedy action\")\n","    \n","  def update_state_history(self, s):\n","    self.state_history.append(s)\n","    \n","  def update(self, env):\n","    reward = env.reward(self.sym)\n","    target = reward\n","    for prev in reversed(self.state_history):\n","      value = self.V[prev] + self.alpha*(target- self.V[prev])\n","      self.V[prev] = value\n","      target = value\n","    self.reset_history()\n","   "],"execution_count":0,"outputs":[]},{"metadata":{"id":"z0vOiNeuveAB","colab_type":"code","colab":{}},"cell_type":"code","source":["class Human:\n","  def __init__(self):\n","    pass\n","  \n","  def get_symbol(self, sym):\n","    self.sym = sym\n","  \n","  def take_action(self, env):\n","    while True:\n","      # break if we make a legal move\n","      move = input(\"Enter i, j for your next move: \")\n","      i, j = = move.split(',')\n","      i = int(i)\n","      j = int(j)\n","      if env.is_empty(i, j):\n","        env.board[i,j] = self.sym\n","        break\n","  def update(self, env):\n","    pass\n","  \n","  def update_state_history(self, s):\n","    pass"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OPGvzQtNwR9b","colab_type":"code","colab":{}},"cell_type":"code","source":["if __name__ == '__main__':\n","  # train the agent\n","  p1 = Agent()\n","  p2 = Agent()\n","  # set initial V for p1 and p2\n","  env = Enviornment()\n","  state_winner_triples = get_state_hash_and_winner(env)\n","  \n","  Vx = initialV_x(env, state_winner_triples)\n","  p1.setV(Vx)\n","  Vo = initialV_o(env, state_winner_triples)\n","  p2.setV(Vo)\n","  \n","  # give each player their symbol\n","  p1.set_symbol(env.x)\n","  p2.set_symbol(env.o)\n","  \n","  T = 10000\n","  \n","  for t in range(T):\n","    if t % 200 == 0:\n","      print(t)\n","    play_game(p1, p2, Enviornment())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qXoTUwGB9Noi","colab_type":"code","colab":{}},"cell_type":"code","source":["human = Human()\n","human.set_symbol(env.o)\n","while True:\n","  p1.set_verbose(True)\n","  play_game(p1, human, Enviornment(), draw=2)\n","  answer = input(\"Play again? [Y/N]: \")\n","  if answer and answer.lower()[0] == 'n':\n","    break"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p9saRnMPAg4U","colab_type":"text"},"cell_type":"markdown","source":["# Final Code"]},{"metadata":{"id":"Q31XTsxIAdKG","colab_type":"code","outputId":"c181bf3b-a4cf-4ccc-d131-515d58c77067","executionInfo":{"status":"ok","timestamp":1549560994954,"user_tz":300,"elapsed":44189,"user":{"displayName":"Michael","photoUrl":"https://lh6.googleusercontent.com/-jQKAMNwL29k/AAAAAAAAAAI/AAAAAAAAATc/K9QNmN92hoY/s64/photo.jpg","userId":"07245055301930887785"}},"colab":{"base_uri":"https://localhost:8080/","height":1858}},"cell_type":"code","source":["# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n","# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n","# Simple reinforcement learning algorithm for learning tic-tac-toe\n","# Use the update rule: V(s) = V(s) + alpha*(V(s') - V(s))\n","# Use the epsilon-greedy policy:\n","#   action|s = argmax[over all actions possible from state s]{ V(s) }  if rand > epsilon\n","#   action|s = select random action from possible actions from state s if rand < epsilon\n","#\n","#\n","# INTERESTING THINGS TO TRY:\n","#\n","# Currently, both agents use the same learning strategy while they play against each other.\n","# What if they have different learning rates?\n","# What if they have different epsilons? (probability of exploring)\n","#   Who will converge faster?\n","# What if one agent doesn't learn at all?\n","#   Poses an interesting philosophical question: If there's no one around to challenge you,\n","#   can you reach your maximum potential?\n","from __future__ import print_function, division\n","from builtins import range, input\n","# Note: you may need to update your version of future\n","# sudo pip install -U future\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","LENGTH = 3\n","\n","\n","class Agent:\n","  def __init__(self, eps=0.1, alpha=0.5):\n","    self.eps = eps # probability of choosing random action instead of greedy\n","    self.alpha = alpha # learning rate\n","    self.verbose = False\n","    self.state_history = []\n","  \n","  def setV(self, V):\n","    self.V = V\n","\n","  def set_symbol(self, sym):\n","    self.sym = sym\n","\n","  def set_verbose(self, v):\n","    # if true, will print values for each position on the board\n","    self.verbose = v\n","\n","  def reset_history(self):\n","    self.state_history = []\n","\n","  def take_action(self, env):\n","    # choose an action based on epsilon-greedy strategy\n","    r = np.random.rand()\n","    best_state = None\n","    if r < self.eps:\n","      # take a random action\n","      if self.verbose:\n","        print(\"Taking a random action\")\n","\n","      possible_moves = []\n","      for i in range(LENGTH):\n","        for j in range(LENGTH):\n","          if env.is_empty(i, j):\n","            possible_moves.append((i, j))\n","      idx = np.random.choice(len(possible_moves))\n","      next_move = possible_moves[idx]\n","    else:\n","      # choose the best action based on current values of states\n","      # loop through all possible moves, get their values\n","      # keep track of the best value\n","      pos2value = {} # for debugging\n","      next_move = None\n","      best_value = -1\n","      for i in range(LENGTH):\n","        for j in range(LENGTH):\n","          if env.is_empty(i, j):\n","            # what is the state if we made this move?\n","            env.board[i,j] = self.sym\n","            state = env.get_state()\n","            env.board[i,j] = 0 # don't forget to change it back!\n","            pos2value[(i,j)] = self.V[state]\n","            if self.V[state] > best_value:\n","              best_value = self.V[state]\n","              best_state = state\n","              next_move = (i, j)\n","\n","      # if verbose, draw the board w/ the values\n","      if self.verbose:\n","        print(\"Taking a greedy action\")\n","        for i in range(LENGTH):\n","          print(\"------------------\")\n","          for j in range(LENGTH):\n","            if env.is_empty(i, j):\n","              # print the value\n","              print(\" %.2f|\" % pos2value[(i,j)], end=\"\")\n","            else:\n","              print(\"  \", end=\"\")\n","              if env.board[i,j] == env.x:\n","                print(\"x  |\", end=\"\")\n","              elif env.board[i,j] == env.o:\n","                print(\"o  |\", end=\"\")\n","              else:\n","                print(\"   |\", end=\"\")\n","          print(\"\")\n","        print(\"------------------\")\n","\n","    # make the move\n","    env.board[next_move[0], next_move[1]] = self.sym\n","\n","  def update_state_history(self, s):\n","    # cannot put this in take_action, because take_action only happens\n","    # once every other iteration for each player\n","    # state history needs to be updated every iteration\n","    # s = env.get_state() # don't want to do this twice so pass it in\n","    self.state_history.append(s)\n","\n","  def update(self, env):\n","    # we want to BACKTRACK over the states, so that:\n","    # V(prev_state) = V(prev_state) + alpha*(V(next_state) - V(prev_state))\n","    # where V(next_state) = reward if it's the most current state\n","    #\n","    # NOTE: we ONLY do this at the end of an episode\n","    # not so for all the algorithms we will study\n","    reward = env.reward(self.sym)\n","    target = reward\n","    for prev in reversed(self.state_history):\n","      value = self.V[prev] + self.alpha*(target - self.V[prev])\n","      self.V[prev] = value\n","      target = value\n","    self.reset_history()\n","\n","\n","# this class represents a tic-tac-toe game\n","# is a CS101-type of project\n","class Environment:\n","  def __init__(self):\n","    self.board = np.zeros((LENGTH, LENGTH))\n","    self.x = -1 # represents an x on the board, player 1\n","    self.o = 1 # represents an o on the board, player 2\n","    self.winner = None\n","    self.ended = False\n","    self.num_states = 3**(LENGTH*LENGTH)\n","\n","  def is_empty(self, i, j):\n","    return self.board[i,j] == 0\n","\n","  def reward(self, sym):\n","    # no reward until game is over\n","    if not self.game_over():\n","      return 0\n","\n","    # if we get here, game is over\n","    # sym will be self.x or self.o\n","    return 1 if self.winner == sym else 0\n","\n","  def get_state(self):\n","    # returns the current state, represented as an int\n","    # from 0...|S|-1, where S = set of all possible states\n","    # |S| = 3^(BOARD SIZE), since each cell can have 3 possible values - empty, x, o\n","    # some states are not possible, e.g. all cells are x, but we ignore that detail\n","    # this is like finding the integer represented by a base-3 number\n","    k = 0\n","    h = 0\n","    for i in range(LENGTH):\n","      for j in range(LENGTH):\n","        if self.board[i,j] == 0:\n","          v = 0\n","        elif self.board[i,j] == self.x:\n","          v = 1\n","        elif self.board[i,j] == self.o:\n","          v = 2\n","        h += (3**k) * v\n","        k += 1\n","    return h\n","\n","  def game_over(self, force_recalculate=False):\n","    # returns true if game over (a player has won or it's a draw)\n","    # otherwise returns false\n","    # also sets 'winner' instance variable and 'ended' instance variable\n","    if not force_recalculate and self.ended:\n","      return self.ended\n","    \n","    # check rows\n","    for i in range(LENGTH):\n","      for player in (self.x, self.o):\n","        if self.board[i].sum() == player*LENGTH:\n","          self.winner = player\n","          self.ended = True\n","          return True\n","\n","    # check columns\n","    for j in range(LENGTH):\n","      for player in (self.x, self.o):\n","        if self.board[:,j].sum() == player*LENGTH:\n","          self.winner = player\n","          self.ended = True\n","          return True\n","\n","    # check diagonals\n","    for player in (self.x, self.o):\n","      # top-left -> bottom-right diagonal\n","      if self.board.trace() == player*LENGTH:\n","        self.winner = player\n","        self.ended = True\n","        return True\n","      # top-right -> bottom-left diagonal\n","      if np.fliplr(self.board).trace() == player*LENGTH:\n","        self.winner = player\n","        self.ended = True\n","        return True\n","\n","    # check if draw\n","    if np.all((self.board == 0) == False):\n","      # winner stays None\n","      self.winner = None\n","      self.ended = True\n","      return True\n","\n","    # game is not over\n","    self.winner = None\n","    return False\n","\n","  def is_draw(self):\n","    return self.ended and self.winner is None\n","\n","  # Example board\n","  # -------------\n","  # | x |   |   |\n","  # -------------\n","  # |   |   |   |\n","  # -------------\n","  # |   |   | o |\n","  # -------------\n","  def draw_board(self):\n","    for i in range(LENGTH):\n","      print(\"-------------\")\n","      for j in range(LENGTH):\n","        print(\"  \", end=\"\")\n","        if self.board[i,j] == self.x:\n","          print(\"x \", end=\"\")\n","        elif self.board[i,j] == self.o:\n","          print(\"o \", end=\"\")\n","        else:\n","          print(\"  \", end=\"\")\n","      print(\"\")\n","    print(\"-------------\")\n","\n","\n","\n","class Human:\n","  def __init__(self):\n","    pass\n","\n","  def set_symbol(self, sym):\n","    self.sym = sym\n","\n","  def take_action(self, env):\n","    while True:\n","      # break if we make a legal move\n","      move = input(\"Enter coordinates i,j for your next move (i,j=0..2): \")\n","      i, j = move.split(',')\n","      i = int(i)\n","      j = int(j)\n","      if env.is_empty(i, j):\n","        env.board[i,j] = self.sym\n","        break\n","\n","  def update(self, env):\n","    pass\n","\n","  def update_state_history(self, s):\n","    pass\n","\n","\n","# recursive function that will return all\n","# possible states (as ints) and who the corresponding winner is for those states (if any)\n","# (i, j) refers to the next cell on the board to permute (we need to try -1, 0, 1)\n","# impossible games are ignored, i.e. 3x's and 3o's in a row simultaneously\n","# since that will never happen in a real game\n","def get_state_hash_and_winner(env, i=0, j=0):\n","  results = []\n","\n","  for v in (0, env.x, env.o):\n","    env.board[i,j] = v # if empty board it should already be 0\n","    if j == 2:\n","      # j goes back to 0, increase i, unless i = 2, then we are done\n","      if i == 2:\n","        # the board is full, collect results and return\n","        state = env.get_state()\n","        ended = env.game_over(force_recalculate=True)\n","        winner = env.winner\n","        results.append((state, winner, ended))\n","      else:\n","        results += get_state_hash_and_winner(env, i + 1, 0)\n","    else:\n","      # increment j, i stays the same\n","      results += get_state_hash_and_winner(env, i, j + 1)\n","\n","  return results\n","\n","# play all possible games\n","# need to also store if game is over or not\n","# because we are going to initialize those values to 0.5\n","# NOTE: THIS IS SLOW because MANY possible games lead to the same outcome / state\n","# def get_state_hash_and_winner(env, turn='x'):\n","#   results = []\n","\n","#   state = env.get_state()\n","#   # board_before = env.board.copy()\n","#   ended = env.game_over(force_recalculate=True)\n","#   winner = env.winner\n","#   results.append((state, winner, ended))\n","\n","#   # DEBUG\n","#   # if ended:\n","#   #   if winner is not None and env.win_type.startswith('col'):\n","#   #     env.draw_board()\n","#   #     print \"Winner:\", 'x' if winner == -1 else 'o', env.win_type\n","#   #     print \"\\n\\n\"\n","#   #     assert(np.all(board_before == env.board))\n","\n","#   if not ended:\n","#     if turn == 'x':\n","#       sym = env.x\n","#       next_sym = 'o'\n","#     else:\n","#       sym = env.o\n","#       next_sym = 'x'\n","\n","#     for i in xrange(LENGTH):\n","#       for j in xrange(LENGTH):\n","#         if env.is_empty(i, j):\n","#           env.board[i,j] = sym\n","#           results += get_state_hash_and_winner(env, next_sym)\n","#           env.board[i,j] = 0 # reset it\n","#   return results\n","\n","\n","def initialV_x(env, state_winner_triples):\n","  # initialize state values as follows\n","  # if x wins, V(s) = 1\n","  # if x loses or draw, V(s) = 0\n","  # otherwise, V(s) = 0.5\n","  V = np.zeros(env.num_states)\n","  for state, winner, ended in state_winner_triples:\n","    if ended:\n","      if winner == env.x:\n","        v = 1\n","      else:\n","        v = 0\n","    else:\n","      v = 0.5\n","    V[state] = v\n","  return V\n","\n","\n","def initialV_o(env, state_winner_triples):\n","  # this is (almost) the opposite of initial V for player x\n","  # since everywhere where x wins (1), o loses (0)\n","  # but a draw is still 0 for o\n","  V = np.zeros(env.num_states)\n","  for state, winner, ended in state_winner_triples:\n","    if ended:\n","      if winner == env.o:\n","        v = 1\n","      else:\n","        v = 0\n","    else:\n","      v = 0.5\n","    V[state] = v\n","  return V\n","\n","\n","def play_game(p1, p2, env, draw=False):\n","  # loops until the game is over\n","  current_player = None\n","  while not env.game_over():\n","    # alternate between players\n","    # p1 always starts first\n","    if current_player == p1:\n","      current_player = p2\n","    else:\n","      current_player = p1\n","\n","    # draw the board before the user who wants to see it makes a move\n","    if draw:\n","      if draw == 1 and current_player == p1:\n","        env.draw_board()\n","      if draw == 2 and current_player == p2:\n","        env.draw_board()\n","\n","    # current player makes a move\n","    current_player.take_action(env)\n","\n","    # update state histories\n","    state = env.get_state()\n","    p1.update_state_history(state)\n","    p2.update_state_history(state)\n","\n","  if draw:\n","    env.draw_board()\n","\n","  # do the value function update\n","  p1.update(env)\n","  p2.update(env)\n","\n","\n","# Game Start\n","if __name__ == '__main__':\n","  # train the agent\n","  p1 = Agent()\n","  p2 = Agent()\n","\n","  # set initial V for p1 and p2\n","  env = Environment()\n","  state_winner_triples = get_state_hash_and_winner(env)\n","\n","\n","  Vx = initialV_x(env, state_winner_triples)\n","  p1.setV(Vx)\n","  Vo = initialV_o(env, state_winner_triples)\n","  p2.setV(Vo)\n","\n","  # give each player their symbol\n","  p1.set_symbol(env.x)\n","  p2.set_symbol(env.o)\n","\n","  T = 10000\n","  for t in range(T):\n","    if t % 200 == 0:\n","      print(t)\n","    play_game(p1, p2, Environment())\n","\n","  # play human vs. agent\n","  # do you think the agent learned to play the game well?\n","  human = Human()\n","  human.set_symbol(env.o)\n","  while True:\n","    p1.set_verbose(True)\n","    play_game(p1, human, Environment(), draw=2)\n","    # I made the agent player 1 because I wanted to see if it would\n","    # select the center as its starting move. If you want the agent\n","    # to go second you can switch the human and AI.\n","    answer = input(\"Play again? [Y/n]: \")\n","    if answer and answer.lower()[0] == 'n':\n","      break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","200\n","400\n","600\n","800\n","1000\n","1200\n","1400\n","1600\n","1800\n","2000\n","2200\n","2400\n","2600\n","2800\n","3000\n","3200\n","3400\n","3600\n","3800\n","4000\n","4200\n","4400\n","4600\n","4800\n","5000\n","5200\n","5400\n","5600\n","5800\n","6000\n","6200\n","6400\n","6600\n","6800\n","7000\n","7200\n","7400\n","7600\n","7800\n","8000\n","8200\n","8400\n","8600\n","8800\n","9000\n","9200\n","9400\n","9600\n","9800\n","Taking a greedy action\n","------------------\n"," 0.63| 0.52| 0.54|\n","------------------\n"," 0.51| 0.90| 0.49|\n","------------------\n"," 0.53| 0.49| 0.61|\n","------------------\n","-------------\n","            \n","-------------\n","      x     \n","-------------\n","            \n","-------------\n","Enter coordinates i,j for your next move (i,j=0..2): 0,1\n","Taking a random action\n","-------------\n","      o     \n","-------------\n","  x   x     \n","-------------\n","            \n","-------------\n","Enter coordinates i,j for your next move (i,j=0..2): 1,2\n","Taking a greedy action\n","------------------\n"," 1.00|  o  | 0.75|\n","------------------\n","  x  |  x  |  o  |\n","------------------\n"," 0.62| 0.16| 0.40|\n","------------------\n","-------------\n","  x   o     \n","-------------\n","  x   x   o \n","-------------\n","            \n","-------------\n","Enter coordinates i,j for your next move (i,j=0..2): 2,0\n","Taking a greedy action\n","------------------\n","  x  |  o  | 0.33|\n","------------------\n","  x  |  x  |  o  |\n","------------------\n","  o  | 0.96| 1.00|\n","------------------\n","-------------\n","  x   o     \n","-------------\n","  x   x   o \n","-------------\n","  o       x \n","-------------\n"],"name":"stdout"}]}]}