{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Section 5: Markov Decision Process.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"3RMuM2Sspey2","colab_type":"text"},"cell_type":"markdown","source":["# Markov Decision Process\n","\n"," * Using a much smaller grid, a robot has to reach a reward.\n"," * Application: DeepMind used concatenation of 4 most recent frames to represent state.\n"," * MDP is defined as the collection of set of states, actions, rewards, state-transition probabilities, reward probabilities, discount factor. \n"," * Policy: The way we make decision for what action to do "]},{"metadata":{"id":"RjEoROWasfkU","colab_type":"text"},"cell_type":"markdown","source":["## Total reward \n","* We are interested in measuring the total future rewards \n","* Everything from t+1 onwards\n","* Discount Factor: Gamma, 1 - don't care how far the future reward is 0 -only try to maximize immediate rewards. \n","* All of the equations should be wrriten in continuous form "]},{"metadata":{"id":"OqclX_yBtWs-","colab_type":"text"},"cell_type":"markdown","source":["## Value Function\n","* A subset of a tree is also a tree -- recursion\n","* At each state S, I will get reward R\n","* G is the sum of all rewards I get\n","* V(s) = E(G | s)\n","* | given - anything to the left is random / right is not random\n","* The value of a state is just the sum of all future rewards \n","* V(s1) = r2 + r3 + r4 + .... Rn\n","* Key: V(s1) = r2 + V(s2)\n","* Q(s, a) = action value function\n","* To understand how Q and s are related, we need to look at the policies "]},{"metadata":{"id":"Bf4GeIZFDkbc","colab_type":"text"},"cell_type":"markdown","source":["## Value functions Advanced\n","\n","* Value function is determined by a policy and has state s as parameter \n","* Only future rewards\n","* We're summing over all the random variables the reward depends on\n","\n","\n","### Bellman Equation\n","* Bottom-up approach \n","* Dynamic Programming is also one of the solutions we'll study for MDP \n"]},{"metadata":{"id":"Hrz-QJ3MIXGK","colab_type":"text"},"cell_type":"markdown","source":["## Bellman Equation \n","\n","* All we want to do is solve for V(s)\n","* Value is sum of all future rewards\n","* The value of the terminal state is always zero\n","* Just marginalize over reward since they are deterministic\n","* Method #1: Working backwards\n","* Method #2: Linear Equations method\n","* The real work is figuring out how the algorithm works and coding it\n","\n"]},{"metadata":{"id":"i-PTJ_6dTMld","colab_type":"text"},"cell_type":"markdown","source":["## Optimal Policies & Optimal Value Functions\n","\n","* These are independent \n","* Optimal Policy is the best policy\n","* The policy for which there is no greatest value function \n","* Optimal policies are not unique, optimal value functions are\n","* To find the best action, we must actually do it to find the best V(s`)\n","* Value function already takes future rewards into account\n","* Choose the action that yields the best next-state value V(s`)\n","* If we have Q(s,a) simply choose argmax "]}]}